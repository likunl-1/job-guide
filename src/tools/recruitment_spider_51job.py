"""
å‰ç¨‹æ— å¿§ï¼ˆ51jobï¼‰æ‹›è˜ä¿¡æ¯çˆ¬è™«å·¥å…·
æ”¯æŒé€šè¿‡å…³é”®è¯å’ŒåŸå¸‚æœç´¢èŒä½ä¿¡æ¯
"""

import re
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote
from langchain.tools import tool

from tools.citynum import city_to_num
from tools.data_saver import DataSaver


@tool
def search_51job(
    keyword: str,
    city: str = "æ·±åœ³",
    max_pages: int = 3
) -> str:
    """
    ä»å‰ç¨‹æ— å¿§ï¼ˆ51jobï¼‰çˆ¬å–æ‹›è˜ä¿¡æ¯
    
    Args:
        keyword: æœç´¢å…³é”®è¯ï¼Œå¦‚"Pythonå¼€å‘"ã€"æ•°æ®åˆ†æå¸ˆ"ç­‰
        city: åŸå¸‚åç§°ï¼Œå¦‚"æ·±åœ³"ã€"åŒ—äº¬"ã€"ä¸Šæµ·"ç­‰
        max_pages: æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆé»˜è®¤3é¡µï¼Œé˜²æ­¢æ•°æ®è¿‡å¤šï¼‰
    
    Returns:
        çˆ¬å–ç»“æœçš„æ‘˜è¦ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ•°æ®æ¡æ•°ã€ä¿å­˜è·¯å¾„ç­‰
    
    Examples:
        >>> search_51job("Pythonå¼€å‘", "æ·±åœ³", 2)
        >>> search_51job("æ•°æ®åˆ†æå¸ˆ", "åŒ—äº¬", 5)
    """
    # è¯·æ±‚å¤´
    headers = {
        "Host": "search.51job.com",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    try:
        # è·å–åŸå¸‚ä»£ç 
        citys = [city]
        citynum = city_to_num.get_citynum(citys)
        
        if citynum == "040000":
            return f"âš ï¸ æ— æ³•åŒ¹é…åŸå¸‚ã€{city}ã€‘ï¼Œå°†ä½¿ç”¨é»˜è®¤åŸå¸‚ã€æ·±åœ³ã€‘"
        
        # æ„é€ æœç´¢ URL
        search_url = (
            f"http://search.51job.com/jobsearch/search_result.php?"
            f"fromJs=1&jobarea={quote(citynum)}&keyword={quote(keyword)}"
            f"&keywordtype=2&lang=c&stype=2&postchannel=0000&fromType=1&confirmdate=9"
        )
        
        # åˆå§‹åŒ–æ•°æ®ä¿å­˜å™¨
        saver = DataSaver(keyword, citys)
        
        # å¼€å§‹çˆ¬å–
        total_jobs = 0
        current_page = 0
        current_url = search_url
        
        while current_page < max_pages:
            print(f"\næ­£åœ¨çˆ¬å–ç¬¬ {current_page + 1} é¡µ...")
            
            try:
                # å‘é€è¯·æ±‚
                response = requests.get(current_url, headers=headers, timeout=10)
                response.encoding = "gbk"
                
                # è§£æ HTML
                soup = BeautifulSoup(response.text, "lxml")
                
                # è·å–èŒä½ä¿¡æ¯ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰
                jobs = soup.select("#resultList > div.el")[1:]
                
                if not jobs:
                    print("æœ¬é¡µæ²¡æœ‰èŒä½ä¿¡æ¯ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                # æå–èŒä½ä¿¡æ¯
                page_jobs = 0
                for job in jobs:
                    try:
                        data = {}
                        
                        # èŒä½åç§°å’Œé“¾æ¥
                        job_info = job.select("p.t1")[0]
                        data["èŒä½åç§°"] = job_info.text.strip()
                        job_link_tag = job_info.select("span > a")
                        if job_link_tag:
                            data["æ‹›è˜é“¾æ¥"] = job_link_tag[0].get("href", "")
                        else:
                            data["æ‹›è˜é“¾æ¥"] = ""
                        
                        # å…¬å¸åç§°å’Œé“¾æ¥
                        company_info = job.select("span.t2")[0]
                        data["å…¬å¸åç§°"] = company_info.text.strip()
                        company_link_tag = company_info.select("a")
                        if company_link_tag:
                            data["å…¬å¸é“¾æ¥"] = company_link_tag[0].get("href", "")
                        else:
                            data["å…¬å¸é“¾æ¥"] = ""
                        
                        # å·¥ä½œåœ°ç‚¹
                        location_tag = job.select("span.t3")
                        data["å·¥ä½œåœ°ç‚¹"] = location_tag[0].text.strip() if location_tag else ""
                        
                        # è–ªèµ„
                        salary_tag = job.select("span.t4")
                        data["è–ªèµ„"] = salary_tag[0].text.strip() if salary_tag else "é¢è®®"
                        
                        # å‘å¸ƒæ—¶é—´
                        date_tag = job.select("span.t5")
                        data["å‘å¸ƒæ—¶é—´"] = date_tag[0].text.strip() if date_tag else ""
                        
                        # ä¿å­˜æ•°æ®
                        saver.insert_data(data)
                        total_jobs += 1
                        page_jobs += 1
                        
                    except Exception as e:
                        print(f"è§£æèŒä½ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                        continue
                
                print(f"ç¬¬ {current_page + 1} é¡µæˆåŠŸçˆ¬å– {page_jobs} ä¸ªèŒä½")
                
                # å°è¯•è·å–ä¸‹ä¸€é¡µé“¾æ¥
                try:
                    next_link_tag = soup.select("li.bk")[-1].select("a")
                    if not next_link_tag:
                        print("æ— æ³•è·å–ä¸‹ä¸€é¡µé“¾æ¥ï¼Œåœæ­¢çˆ¬å–")
                        break
                    
                    next_link = next_link_tag[0].get("href")
                    
                    if next_link is None:
                        print("å·²åˆ°è¾¾æœ€åä¸€é¡µï¼Œåœæ­¢çˆ¬å–")
                        break
                    
                    if "javascript:" in next_link:
                        print("å·²åˆ°è¾¾æœ€åä¸€é¡µï¼Œåœæ­¢çˆ¬å–")
                        break
                    
                    current_url = next_link
                    current_page += 1
                    time.sleep(1)  # å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                except:
                    print("æ— æ³•è·å–ä¸‹ä¸€é¡µé“¾æ¥ï¼Œåœæ­¢çˆ¬å–")
                    break
                    
            except requests.RequestException as e:
                print(f"è¯·æ±‚å‡ºé”™: {e}")
                break
            except Exception as e:
                print(f"çˆ¬å–å‡ºé”™: {e}")
                break
        
        # ä¿å­˜æ•°æ®
        saved_file = saver.save()
        
        # ç”Ÿæˆè¿”å›ç»“æœ
        result = f"""
## ğŸ“Š çˆ¬å–ç»“æœæ‘˜è¦

**æœç´¢å…³é”®è¯**: {keyword}
**æœç´¢åŸå¸‚**: {city}
**çˆ¬å–é¡µæ•°**: {current_page + 1} é¡µ
**æ•°æ®æ¡æ•°**: {total_jobs} æ¡

**ä¿å­˜è·¯å¾„**: {saved_file if saved_file else 'ä¿å­˜å¤±è´¥'}

### ğŸ” æ•°æ®å­—æ®µ
- èŒä½åç§°
- å…¬å¸åç§°
- è–ªèµ„
- å·¥ä½œåœ°ç‚¹
- å‘å¸ƒæ—¶é—´
- æ‹›è˜é“¾æ¥
- å…¬å¸é“¾æ¥

### âœ… è¯´æ˜
æ•°æ®å·²ä¿å­˜åˆ° `assets/jobs_data/` ç›®å½•ï¼Œå¯ä»¥ä½¿ç”¨ `read_local_jobs` å·¥å…·è¯»å–æ•°æ®ã€‚
"""
        return result
        
    except Exception as e:
        return f"âŒ çˆ¬å–å¤±è´¥: {str(e)}\n\nè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"


# å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œå¯ä»¥æµ‹è¯•çˆ¬è™«
if __name__ == '__main__':
    print("æµ‹è¯•å‰ç¨‹æ— å¿§çˆ¬è™«...")
    result = search_51job("Pythonå¼€å‘", "æ·±åœ³", 1)
    print(result)
